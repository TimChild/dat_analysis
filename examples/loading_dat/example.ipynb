{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Covered?\n",
    "- A few different options for how to load a datXXX.h5 file with dat_analysis (loading a DatHDF object)\n",
    "- Some notes/tips about working with DatHDF objects\n",
    "\n",
    "### Note \n",
    "In general use, you will not have to think about most of this as you'll set something like the `get_dat` function below once, and then just use that from then on.\n",
    "\n",
    "For use on any new computer, there are a couple of things that need to be set up in a `config.toml` to enable the most general use of `dat_analysis`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dat_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dat_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from explicit filepath\n",
    "I.e. You want to load a specific `datXXX.h5` file and you know the full absolute or relative path to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dat6420 - March 3, 2023 14:35:42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_experiment_dat = 'experiment_dats/dat6420.h5'\n",
    "path_to_save_location = 'analyzed_dats/dat6420.h5'  # This is not necessary when the config.toml file is set-up. But a good idea when explicitly loading a single dat like this\n",
    "dat = dat_analysis.get_dat_from_exp_filepath(path_to_experiment_dat, override_save_path=path_to_save_location, overwrite=False)\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving the `dat` object as the last output of a cell, we can see that we have successfully loaded an experiment file and the date and time it was saved.\n",
    "\n",
    "This dat object is an instance of `DatHDF` which provides easy access information stored in the `.h5` file. See other examples for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dat_analysis.dat.dat_hdf.DatHDF"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from measurement-data\n",
    "Generally, you'll be loading dats that are saved on the server under the `measurement-data` folder. \n",
    "\n",
    "If a path to this folder is set up in the `config.toml` then you can more easily access any dat file.\n",
    "\n",
    "**Note: I'll leave these examples commented out since you may not already have the config.toml set-up**\n",
    "\n",
    "The file structure of `measurement-data` is `measurement-data/<host>/<user>/<experiment>/datXXX.h5`. `dat_analysis` assumes it has a path to `measurement-data` already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "host_name = 'qdev-xld'  # The name of the Measurement PC \n",
    "user_name = 'Tim'  # The name of the user \n",
    "experiment_name = '202211_KondoEntropy'  # Experiment path. Note: this can include more subpaths if necessary e.g. '2023/OhmicTests/Test1/'\n",
    "# dat = dat_analysis.get_dat(6420, host_name, user_name, experiment_name, raw=False, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `raw` is related to FastDAC data and allows to easily switch between loading the resampled or raw data that Igor saves when resampling is on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Setup\n",
    "\n",
    "In general, when using this analysis for an experiment, you would define a `get_dat` function in a `.py` file that you can easily import from so that you can easily access the dats for your current experiment from their datnum alone without having to specify everything else every time.\n",
    "\n",
    "**Note: In this example, I'll specify paths to the example folders here, but if the `config.toml` is set-up, you could use the `dat_analysis.get_dat(...)` function instead as noted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dat_analysis\n",
    "\n",
    "def get_dat(datnum, raw=False, overwrite=False):\n",
    "    \"\"\"\n",
    "    Define a simple get_dat function that knows where to load dats from and save them to\n",
    "\n",
    "    Note: In a full setup where a path to `measurement-data` and a general save location have already been set in the dat_analysis config.toml, you can use:\n",
    "        return dat_analysis.get_dat(datnum, host_name, user_name, experiment_name, raw=raw, overwrite=overwrite)\n",
    "    \"\"\"\n",
    "    hdf_name = f\"dat{datnum}{'_RAW' if raw else ''}.h5\"\n",
    "    return dat_analysis.get_dat_from_exp_filepath(f'experiment_dats/{hdf_name}', override_save_path=f'analyzed_dats/{hdf_name}', overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dat6420 - March 3, 2023 14:35:42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = get_dat(6420)\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the other examples, we'll start with something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on working with DatHDF object\n",
    "\n",
    "- `dat_analysis` is written such that when you \"load\" a dat file, you don't actually load much data from the disk until you actually need it. The dat object is mostly an interface for the standard HDF file that `dat_analysis` creates when first loading an experiment dat.\n",
    "- the HDF that you are actually working with is separate to the experiment `.h5` file, so you don't need to worry about deleting or modifying your experiment data\n",
    "    - Additionally, if something goes wrong with the datHDF file, you can just load it again with `overwrite=True` and you'll have a clean slate copy from the experiment `.h5` file again.\n",
    "- The access to the underlying HDF file is (almostly completely) threadsafe and process safe\n",
    "    - i.e. If you really want to, you should be able to use mutiple threads/processes to access a single dat file (it's not 100% perfect)\n",
    "    - This is mostly intended for use with `Dash` that ends up doing a lot of muli-threaded/processed operations\n",
    "- Not everything in the HDF file has a super easy access from the DatHDF object\n",
    "    - But, the DatHDF object does provide some easy access to the HDF file for reading or writing (see below)\n",
    "- You can save anything else you like in the HDF file\n",
    "    - There are some helper functions for saving/loading things to/from HDF in `dat_analysis.hdf_util`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with the HDF directly\n",
    "\n",
    "If you want to interact with the analyzed HDF file directly, you should always access it via the `DatHDF` object to avoid file access issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['Data', 'Logs']>\n",
      "<KeysViewHDF5 ['datnum', 'experiment_data_path', 'new_attr']>\n"
     ]
    }
   ],
   "source": [
    "# To read from the HDF:\n",
    "with dat.hdf_read as f:\n",
    "    print(f.keys())\n",
    "    print(f.attrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "# To write to HDF:\n",
    "with dat.hdf_write as f:\n",
    "    f.attrs['new_attr'] = 'test'\n",
    "\n",
    "with dat.hdf_read as f:\n",
    "    attr_val = f.attrs['new_attr']\n",
    "print(attr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
